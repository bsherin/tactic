Do you think this type of analysis will work on different types of data? If so, what kind of topics do you see it being used for in the future? If not, why don’t you think so?
Is there a certain technique to choosing the appropriate number of clusters? In the article, Sherin chose 7 clusters.  He also mentioned the more clusters there are the more detailed the data, but that also makes it  more complicated to work with.  I was just wondering if the number of clusters chosen is arbitrary or there is a systematic process involved.  For instance does it depend on the quality and quantity of the data? Or does one always choose a cluster in the upper middle set of clusters?
Reading this paper did also leave me with a bit of a "so what" feeling that was not touched upon until the conclusion.  Basically what is the extended application of the models described in this paper?  I understand that the paper was to prove systematic purposes in the process of data analysis, but in what way can this be applied to human memory and learning?  More simply, how can a computational model be compared to the human thinking process? 
Do you think that in giving the interviewers freedom to craft questions for clarification, it could have influenced the students or provided inconsistencies in the responses?
